{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp1ELm57Zz2a"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 9207,
     "status": "ok",
     "timestamp": 1677083550034,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "vXCty5i0Zz2b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pyMLaux import show_img_data, plot_history, evaluate_classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'splits/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1108,
     "status": "ok",
     "timestamp": 1677083558884,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "WvZsf7gkPSOY",
    "outputId": "c2fdd292-2aa3-4465-d295-4ca271aff53c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c9KhDatZz2o"
   },
   "source": [
    "## Load Interpolation and Extrapolation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1414,
     "status": "ok",
     "timestamp": 1677083569578,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "NMbX-NupZz2p"
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"splits/training.csv\")\n",
    "training.drop([\"index_number\"], axis=1, inplace=True)\n",
    "training\n",
    "\n",
    "X_train = training.iloc[:, :4]\n",
    "y_train = training.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIRr1M4iZz2r"
   },
   "source": [
    "## Random Search for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1677083593130,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "YJd7EuX0bLSG"
   },
   "outputs": [],
   "source": [
    "def create_hyperparams(n, \n",
    "                       lr, \n",
    "                       epoch_options, \n",
    "                       no_layers_options, \n",
    "                       hidden_layer_options,\n",
    "                       dropout_options, \n",
    "                       activation_options):\n",
    "    df = pd.DataFrame(index=range(n),\n",
    "                      columns=['no_hidden_layers', 'hidden_layers', 'activation', 'dropout', 'lr', 'epochs'])\n",
    "\n",
    "    for i in range(n):\n",
    "        df.loc[i, 'lr'] = lr * 5.**random.uniform(-2., 2.)\n",
    "        df.loc[i, 'epochs'] = random.sample(epoch_options, 1)[0]\n",
    "    \n",
    "        no_layers = int(random.sample(no_hidden_layers, 1)[0])\n",
    "        df.loc[i, 'no_hidden_layers'] = no_layers\n",
    "        df.loc[i, 'hidden_layers'] = sorted([int(random.sample(hidden_layer_options, 1)[0]) for i in range(no_layers)], reverse=True)\n",
    "        df.loc[i, 'dropout'] = random.sample(dropout_options, 1)[0]\n",
    "        df.loc[i, 'activation'] = random.sample(activation_options, 1)[0]\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1677083606711,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "fFtzoyAFbaTf"
   },
   "outputs": [],
   "source": [
    "def create_network(hp, no_inputs, no_outputs, output_activation='linear', **kwargs):\n",
    "    hidden_layers = hp['hidden_layers']\n",
    "    \n",
    "    dropout = hp['dropout']\n",
    "    hidden_activation = hp['activation']\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(no_inputs, )))\n",
    "\n",
    "    for cl in hidden_layers:\n",
    "        model.add(tf.keras.layers.Dense(cl, activation=hidden_activation))\n",
    "        if dropout > 0:\n",
    "            model.add(tf.keras.layers.Dropout(dropout))\n",
    "            \n",
    "    model.add(tf.keras.layers.Dense(no_outputs, activation=output_activation)) \n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=hp['lr'])\n",
    "\n",
    "    model.compile(optimizer=opt, **kwargs)\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1677083644625,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "KxQzPx_-bpl9"
   },
   "outputs": [],
   "source": [
    "def find_best(df, crit='ACC'):\n",
    "    index = np.where(df[crit] == np.amax(df[crit]))[0]\n",
    "    return(df.iloc[list(index), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2zsFjuDbrDW"
   },
   "source": [
    "## Perform Model Selection and Determine Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set of 100 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1677083653798,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "D0tU9QQKbvb8"
   },
   "outputs": [],
   "source": [
    "random.seed(4232)\n",
    "batch_size = 32\n",
    "no_models = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1677083659647,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "0_CsQAjUbzws"
   },
   "outputs": [],
   "source": [
    "model_sel = create_hyperparams(no_models, \n",
    "                               0.001, \n",
    "                               [32, 64], \n",
    "                               [2, 3], \n",
    "                               [64, 128, 256],\n",
    "                               [0.2, 0.3], \n",
    "                               ['relu'])\n",
    "model_sel['MSE'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1677083662728,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "W7CmdoUuzMvD",
    "outputId": "2140bc42-35c8-4795-f611-76b1845c9f18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.006553</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[128]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01994</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[128]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  no_hidden_layers hidden_layers activation dropout        lr epochs  MSE\n",
       "0                3         [256]       relu     0.2  0.000714     64    0\n",
       "1                2            []       relu     0.2  0.000052     64    0\n",
       "2                2            []       relu     0.3  0.006553     32    0\n",
       "3                3         [128]       relu     0.3   0.01994     64    0\n",
       "4                3         [128]       relu     0.3  0.016225     32    0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=4232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "96f90d35de194292a9aac81de1a44fb7",
      "6e1925a876534f50bb242cbcf0940ab6",
      "dfc48cbf68cb4294951b0d48e7d0242c",
      "16acc394f36d4f62bbc116ff572e5bfb",
      "55901904beb64afab291086ae9b38327",
      "ebb9c2065ba341f794d33c5583f12010",
      "15bf4c1be6fe44928d201f1dea9d485f",
      "be98efb613024054adea69a32b6e6e3a",
      "3f541caf30464caba86dc1d31c50a736",
      "d19875237b2e4337abed07843571303e",
      "5272552405754f1f85a290c8f65885fe"
     ]
    },
    "executionInfo": {
     "elapsed": 449814,
     "status": "ok",
     "timestamp": 1677084214029,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "mPYnYCYKb2Ug",
    "outputId": "c5c4793c-f19e-4604-a181-ae23c8a06100",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63af04e3772445ea2dd681ba7dd424a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/648\n",
      "1682/1682 - 4s - loss: 0.0872 - mean_squared_error: 0.0872 - 4s/epoch - 2ms/step\n",
      "Epoch 2/648\n",
      "1682/1682 - 4s - loss: 0.0324 - mean_squared_error: 0.0324 - 4s/epoch - 2ms/step\n",
      "Epoch 3/648\n",
      "1682/1682 - 4s - loss: 0.0294 - mean_squared_error: 0.0294 - 4s/epoch - 2ms/step\n",
      "Epoch 4/648\n",
      "1682/1682 - 4s - loss: 0.0247 - mean_squared_error: 0.0247 - 4s/epoch - 2ms/step\n",
      "Epoch 5/648\n",
      "1682/1682 - 4s - loss: 0.0239 - mean_squared_error: 0.0239 - 4s/epoch - 2ms/step\n",
      "Epoch 6/648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(no_models)):\n\u001b[0;32m      6\u001b[0m     model \u001b[38;5;241m=\u001b[39m create_network(model_sel\u001b[38;5;241m.\u001b[39miloc[i], no_inputs\u001b[38;5;241m=\u001b[39mX_train_sel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      7\u001b[0m                            no_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      8\u001b[0m                            metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMeanSquaredError()])\n\u001b[1;32m---> 11\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_sel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_sel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_sel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x\u001b[38;5;241m=\u001b[39mX_test_sel, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     18\u001b[0m     model_sel\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mean_squared_error(y_test_sel, pred)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLTeaching\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLTeaching\\lib\\site-packages\\keras\\engine\\training.py:1555\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   1552\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_step \u001b[38;5;241m=\u001b[39m data_handler\u001b[38;5;241m.\u001b[39m_initial_step \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1553\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_load_initial_step_from_ckpt()\n\u001b[0;32m   1554\u001b[0m     )\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   1556\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m         ):\n\u001b[0;32m   1563\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLTeaching\\lib\\site-packages\\keras\\engine\\data_adapter.py:1374\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1374\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1375\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1376\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[0;32m   1379\u001b[0m )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLTeaching\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    636\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    638\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    639\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLTeaching\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:725\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;124;03m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 725\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39midentity(value)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLTeaching\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:704\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    702\u001b[0m       result \u001b[38;5;241m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    707\u001b[0m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    708\u001b[0m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m   tape\u001b[38;5;241m.\u001b[39mrecord_operation(\n\u001b[0;32m    710\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle],\n\u001b[0;32m    711\u001b[0m       backward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    712\u001b[0m       forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLTeaching\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:694\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat\u001b[38;5;241m.\u001b[39mforward_compatible(\u001b[38;5;241m2022\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    693\u001b[0m   gen_resource_variable_ops\u001b[38;5;241m.\u001b[39mdisable_copy_on_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m--> 694\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    696\u001b[0m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, result)\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MLTeaching\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:524\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    523\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadVariableOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    527\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kfold.split(X_train):\n",
    "    X_train_sel, X_test_sel = X_train.loc[train_index], X_train.loc[test_index]\n",
    "    y_train_sel, y_test_sel = y_train.loc[train_index], y_train.loc[test_index]\n",
    "\n",
    "    for i in tqdm(range(no_models)):\n",
    "        model = create_network(model_sel.iloc[i], no_inputs=X_train_sel.shape[1],\n",
    "                               no_outputs=1, loss='mse', \n",
    "                               metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "\n",
    "        history = model.fit(x=X_train_sel, y=y_train_sel, \n",
    "                            epochs=model_sel['epochs'][i],\n",
    "                            batch_size=batch_size, \n",
    "                            verbose=2)\n",
    "\n",
    "        pred = model.predict(x=X_test_sel, verbose=0)\n",
    "        \n",
    "        model_sel.loc[i, 'MSE'] += mean_squared_error(y_test_sel, pred)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "    model_sel.loc[i, 'MSE'] /= n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 1419,
     "status": "ok",
     "timestamp": 1677084270815,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "jo0QXrbPnK_N",
    "outputId": "e1e5b975-981a-4129-ef61-54009efc196b"
   },
   "outputs": [],
   "source": [
    "model_sel.sort_values(by='MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1677084278492,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "oKRrMA27ciRn",
    "outputId": "331879d1-5c7e-4db6-e67c-d857477c0723"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 256, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>256</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>256</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_hidden_layers        hidden_layers activation dropout        lr epochs  \\\n",
       "38                4  [512, 256, 256, 64]       relu    0.25  0.000113    256   \n",
       "97                3        [512, 64, 64]       relu    0.25  0.000766    256   \n",
       "\n",
       "         ACC  \n",
       "38  0.955556  \n",
       "97  0.955556  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best(model_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1677084282946,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "W7C3g4pHcG67"
   },
   "outputs": [],
   "source": [
    "best_index = find_best(model_sel).index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow down to 50\n",
    "The best 10 models before, only rarely featured only 1 hidden layer, and only used RELU as an activation function. Also, 32 Epochs were never used. I'd therefor remove these options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_models = 50\n",
    "model_sel = create_hyperparams(no_models, \n",
    "                               0.001, \n",
    "                               [128, 256], \n",
    "                               [2, 3, 4], \n",
    "                               [64, 256, 512],\n",
    "                               [0.2, 0.25, 0.3], \n",
    "                               ['relu'])\n",
    "model_sel['ACC'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 256, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[256, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00168</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.021843</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 256, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 256, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 256, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>[512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>[256, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.008993</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 512, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>[256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>[512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 512, 512, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 512, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 512, 512, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.005553</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>[256, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 512, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.006418</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 512, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 256, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 512, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.018461</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.021523</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>[512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 256, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>[256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 256, 256, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>[64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 512, 512, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>[512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>[256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 256, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00085</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>[512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 512, 256, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.007321</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>[256, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.01341</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 512, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.022258</td>\n",
       "      <td>256</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.016378</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_hidden_layers         hidden_layers activation dropout        lr epochs  \\\n",
       "0                 1                 [256]       relu     0.3  0.000331    256   \n",
       "1                 4    [512, 256, 64, 64]       relu     0.3  0.000416    256   \n",
       "2                 2              [64, 64]       relu    0.25  0.000056    256   \n",
       "3                 3        [256, 256, 64]       relu     0.2  0.000277    128   \n",
       "4                 1                 [512]       relu     0.2   0.00168    256   \n",
       "5                 3         [512, 64, 64]       relu    0.25  0.004181    256   \n",
       "6                 2             [512, 64]       relu    0.25  0.021843    256   \n",
       "7                 3       [512, 256, 256]       relu    0.25  0.000103    256   \n",
       "8                 2             [512, 64]       relu    0.25  0.000719    256   \n",
       "9                 1                  [64]       relu     0.2  0.000696    256   \n",
       "10                4    [512, 256, 64, 64]       relu    0.25  0.000796    256   \n",
       "11                4    [512, 256, 64, 64]       relu     0.3  0.000866    256   \n",
       "12                1                 [512]       relu     0.2  0.000081    128   \n",
       "13                3        [256, 256, 64]       relu    0.25  0.008993    128   \n",
       "14                4    [512, 512, 64, 64]       relu     0.2  0.000172    128   \n",
       "15                1                 [256]       relu     0.3  0.000121    256   \n",
       "16                1                 [512]       relu     0.2  0.002774    128   \n",
       "17                4   [512, 512, 512, 64]       relu    0.25  0.000068    128   \n",
       "18                4   [512, 512, 256, 64]       relu     0.2  0.005072    256   \n",
       "19                4  [512, 512, 512, 256]       relu     0.2  0.000126    128   \n",
       "20                2            [256, 256]       relu     0.3  0.003635    256   \n",
       "21                3        [512, 256, 64]       relu     0.3  0.006097    256   \n",
       "22                2             [256, 64]       relu     0.3  0.005553    256   \n",
       "23                3        [256, 256, 64]       relu     0.2  0.000205    256   \n",
       "24                3        [512, 512, 64]       relu    0.25  0.006418    128   \n",
       "25                4    [512, 512, 64, 64]       relu     0.3  0.003603    128   \n",
       "26                4    [512, 256, 64, 64]       relu     0.2  0.000047    256   \n",
       "27                3        [512, 512, 64]       relu    0.25  0.018461    256   \n",
       "28                3         [512, 64, 64]       relu     0.2  0.021523    256   \n",
       "29                1                 [512]       relu    0.25  0.004529    128   \n",
       "30                4   [512, 256, 256, 64]       relu    0.25  0.000746    256   \n",
       "31                1                 [256]       relu     0.3  0.000946    256   \n",
       "32                4  [512, 256, 256, 256]       relu     0.3  0.000446    256   \n",
       "33                3         [512, 64, 64]       relu     0.2  0.004146    128   \n",
       "34                2            [512, 256]       relu     0.3  0.000842    256   \n",
       "35                1                  [64]       relu    0.25  0.000048    256   \n",
       "36                4   [512, 512, 512, 64]       relu     0.2  0.002316    256   \n",
       "37                2             [512, 64]       relu     0.3  0.000441    128   \n",
       "38                1                 [512]       relu     0.2  0.000781    256   \n",
       "39                1                 [256]       relu    0.25  0.002875    256   \n",
       "40                3       [512, 256, 256]       relu     0.3   0.00085    128   \n",
       "41                1                 [512]       relu     0.2  0.001038    256   \n",
       "42                4  [512, 512, 256, 256]       relu     0.3   0.00011    128   \n",
       "43                3        [512, 256, 64]       relu     0.3  0.007209    128   \n",
       "44                3         [512, 64, 64]       relu     0.3  0.007321    128   \n",
       "45                3         [256, 64, 64]       relu    0.25   0.01341    128   \n",
       "46                3        [512, 256, 64]       relu     0.2  0.000863    256   \n",
       "47                3         [512, 64, 64]       relu     0.3  0.003552    256   \n",
       "48                3       [512, 512, 256]       relu     0.3  0.022258    256   \n",
       "49                2              [64, 64]       relu     0.2  0.016378    128   \n",
       "\n",
       "    ACC  \n",
       "0    -1  \n",
       "1    -1  \n",
       "2    -1  \n",
       "3    -1  \n",
       "4    -1  \n",
       "5    -1  \n",
       "6    -1  \n",
       "7    -1  \n",
       "8    -1  \n",
       "9    -1  \n",
       "10   -1  \n",
       "11   -1  \n",
       "12   -1  \n",
       "13   -1  \n",
       "14   -1  \n",
       "15   -1  \n",
       "16   -1  \n",
       "17   -1  \n",
       "18   -1  \n",
       "19   -1  \n",
       "20   -1  \n",
       "21   -1  \n",
       "22   -1  \n",
       "23   -1  \n",
       "24   -1  \n",
       "25   -1  \n",
       "26   -1  \n",
       "27   -1  \n",
       "28   -1  \n",
       "29   -1  \n",
       "30   -1  \n",
       "31   -1  \n",
       "32   -1  \n",
       "33   -1  \n",
       "34   -1  \n",
       "35   -1  \n",
       "36   -1  \n",
       "37   -1  \n",
       "38   -1  \n",
       "39   -1  \n",
       "40   -1  \n",
       "41   -1  \n",
       "42   -1  \n",
       "43   -1  \n",
       "44   -1  \n",
       "45   -1  \n",
       "46   -1  \n",
       "47   -1  \n",
       "48   -1  \n",
       "49   -1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d29f0d5ca4422a86c78abefa6156be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(no_models)):\n",
    "    model = create_network(model_sel.iloc[i], no_inputs=X_train.shape[1],\n",
    "                           no_outputs=10, loss='sparse_categorical_crossentropy', \n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x=X_train, y=y_train, \n",
    "                        epochs=model_sel['epochs'][i],\n",
    "                        batch_size=batch_size, \n",
    "                        verbose=0)\n",
    "\n",
    "    pred = model.predict(x=X_val, verbose=0)\n",
    "    predC = np.argmax(pred, axis=1)\n",
    "\n",
    "    model_sel.loc[i, 'ACC'] = accuracy_score(y_val, predC)\n",
    "\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 256, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>256</td>\n",
       "      <td>0.957778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>128</td>\n",
       "      <td>0.956667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>256</td>\n",
       "      <td>0.956667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>[512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>256</td>\n",
       "      <td>0.956667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 256, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>256</td>\n",
       "      <td>0.956667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 256, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00085</td>\n",
       "      <td>128</td>\n",
       "      <td>0.956667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>[256, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>256</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00168</td>\n",
       "      <td>256</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>[512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>256</td>\n",
       "      <td>0.954444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 512, 512, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>128</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_hidden_layers        hidden_layers activation dropout        lr epochs  \\\n",
       "10                4   [512, 256, 64, 64]       relu    0.25  0.000796    256   \n",
       "37                2            [512, 64]       relu     0.3  0.000441    128   \n",
       "8                 2            [512, 64]       relu    0.25  0.000719    256   \n",
       "38                1                [512]       relu     0.2  0.000781    256   \n",
       "1                 4   [512, 256, 64, 64]       relu     0.3  0.000416    256   \n",
       "40                3      [512, 256, 256]       relu     0.3   0.00085    128   \n",
       "23                3       [256, 256, 64]       relu     0.2  0.000205    256   \n",
       "4                 1                [512]       relu     0.2   0.00168    256   \n",
       "41                1                [512]       relu     0.2  0.001038    256   \n",
       "17                4  [512, 512, 512, 64]       relu    0.25  0.000068    128   \n",
       "\n",
       "         ACC  \n",
       "10  0.957778  \n",
       "37  0.956667  \n",
       "8   0.956667  \n",
       "38  0.956667  \n",
       "1   0.956667  \n",
       "40  0.956667  \n",
       "23  0.955556  \n",
       "4   0.955556  \n",
       "41  0.954444  \n",
       "17  0.953333  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.sort_values(by='ACC', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>[512, 256, 64, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>256</td>\n",
       "      <td>0.957778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_hidden_layers       hidden_layers activation dropout        lr epochs  \\\n",
       "10                4  [512, 256, 64, 64]       relu    0.25  0.000796    256   \n",
       "\n",
       "         ACC  \n",
       "10  0.957778  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best(model_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = find_best(model_sel).index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3gGoxx4cldW"
   },
   "source": [
    "## Train Model on Entire Training Set Using Best Parameters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1677084292891,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "dHrP42SjcoGa"
   },
   "outputs": [],
   "source": [
    "model = create_network(model_sel.loc[best_index], no_inputs=X_train.shape[1],\n",
    "                       no_outputs=10, loss='sparse_categorical_crossentropy', \n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677084297286,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "GevKGfVpcqUe",
    "outputId": "361987f5-55eb-4294-e72c-5035b8e1430e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               12800     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 165,386\n",
      "Trainable params: 165,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11978,
     "status": "ok",
     "timestamp": 1677084316373,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "8SOr03A1ctap",
    "outputId": "a7421921-142e-4f64-f945-96c40c96478b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 1.9176 - accuracy: 0.3143\n",
      "Epoch 2/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.0956 - accuracy: 0.6317\n",
      "Epoch 3/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.7650 - accuracy: 0.7573\n",
      "Epoch 4/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.6489 - accuracy: 0.8027\n",
      "Epoch 5/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.8347\n",
      "Epoch 6/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.4705 - accuracy: 0.8573\n",
      "Epoch 7/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.4153 - accuracy: 0.8717\n",
      "Epoch 8/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.8830\n",
      "Epoch 9/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3556 - accuracy: 0.8897\n",
      "Epoch 10/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3362 - accuracy: 0.8993\n",
      "Epoch 11/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.3142 - accuracy: 0.9070\n",
      "Epoch 12/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2753 - accuracy: 0.9200\n",
      "Epoch 13/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.9140\n",
      "Epoch 14/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2662 - accuracy: 0.9233\n",
      "Epoch 15/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.9240\n",
      "Epoch 16/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.9240\n",
      "Epoch 17/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2520 - accuracy: 0.9260\n",
      "Epoch 18/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9373\n",
      "Epoch 19/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2115 - accuracy: 0.9360\n",
      "Epoch 20/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1999 - accuracy: 0.9433\n",
      "Epoch 21/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1938 - accuracy: 0.9403\n",
      "Epoch 22/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1791 - accuracy: 0.9447\n",
      "Epoch 23/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.9403\n",
      "Epoch 24/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1769 - accuracy: 0.9457\n",
      "Epoch 25/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1609 - accuracy: 0.9523\n",
      "Epoch 26/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9520\n",
      "Epoch 27/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1506 - accuracy: 0.9520\n",
      "Epoch 28/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1580 - accuracy: 0.9503\n",
      "Epoch 29/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9480\n",
      "Epoch 30/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1342 - accuracy: 0.9597\n",
      "Epoch 31/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1409 - accuracy: 0.9563\n",
      "Epoch 32/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1331 - accuracy: 0.9627\n",
      "Epoch 33/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1527 - accuracy: 0.9533\n",
      "Epoch 34/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9517\n",
      "Epoch 35/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.9620\n",
      "Epoch 36/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1320 - accuracy: 0.9573\n",
      "Epoch 37/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1295 - accuracy: 0.9590\n",
      "Epoch 38/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1117 - accuracy: 0.9680\n",
      "Epoch 39/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1052 - accuracy: 0.9683\n",
      "Epoch 40/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1005 - accuracy: 0.9683\n",
      "Epoch 41/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1130 - accuracy: 0.9630\n",
      "Epoch 42/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1004 - accuracy: 0.9693\n",
      "Epoch 43/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0850 - accuracy: 0.9743\n",
      "Epoch 44/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1190 - accuracy: 0.9603\n",
      "Epoch 45/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0994 - accuracy: 0.9697\n",
      "Epoch 46/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.9690\n",
      "Epoch 47/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1062 - accuracy: 0.9683\n",
      "Epoch 48/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0917 - accuracy: 0.9747\n",
      "Epoch 49/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0974 - accuracy: 0.9683\n",
      "Epoch 50/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9703\n",
      "Epoch 51/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0845 - accuracy: 0.9747\n",
      "Epoch 52/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0920 - accuracy: 0.9737\n",
      "Epoch 53/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9820\n",
      "Epoch 54/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0861 - accuracy: 0.9743\n",
      "Epoch 55/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9723\n",
      "Epoch 56/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0886 - accuracy: 0.9737\n",
      "Epoch 57/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.9767\n",
      "Epoch 58/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0957 - accuracy: 0.9700\n",
      "Epoch 59/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.9733\n",
      "Epoch 60/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.9770\n",
      "Epoch 61/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9780\n",
      "Epoch 62/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9740\n",
      "Epoch 63/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9767\n",
      "Epoch 64/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9823\n",
      "Epoch 65/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9787\n",
      "Epoch 66/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0784 - accuracy: 0.9747\n",
      "Epoch 67/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1032 - accuracy: 0.9693\n",
      "Epoch 68/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.9790\n",
      "Epoch 69/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9820\n",
      "Epoch 70/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9817\n",
      "Epoch 71/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.9787\n",
      "Epoch 72/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9870\n",
      "Epoch 73/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9810\n",
      "Epoch 74/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9803\n",
      "Epoch 75/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9860\n",
      "Epoch 76/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9773\n",
      "Epoch 77/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9827\n",
      "Epoch 78/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9803\n",
      "Epoch 79/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.9757\n",
      "Epoch 80/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9780\n",
      "Epoch 81/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9817\n",
      "Epoch 82/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9843\n",
      "Epoch 83/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9810\n",
      "Epoch 84/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9787\n",
      "Epoch 85/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9800\n",
      "Epoch 86/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9823\n",
      "Epoch 87/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9813\n",
      "Epoch 88/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.9773\n",
      "Epoch 89/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9863\n",
      "Epoch 90/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9793\n",
      "Epoch 91/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9817\n",
      "Epoch 92/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9843\n",
      "Epoch 93/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9860\n",
      "Epoch 94/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0403 - accuracy: 0.9870\n",
      "Epoch 95/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 0.9893\n",
      "Epoch 96/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9847\n",
      "Epoch 97/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0400 - accuracy: 0.9873\n",
      "Epoch 98/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9810\n",
      "Epoch 99/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0431 - accuracy: 0.9847\n",
      "Epoch 100/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0424 - accuracy: 0.9853\n",
      "Epoch 101/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9867\n",
      "Epoch 102/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9847\n",
      "Epoch 103/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9870\n",
      "Epoch 104/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9843\n",
      "Epoch 105/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9857\n",
      "Epoch 106/256\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 0.9850\n",
      "Epoch 107/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9827\n",
      "Epoch 108/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9810\n",
      "Epoch 109/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9893\n",
      "Epoch 110/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9813\n",
      "Epoch 111/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.9907\n",
      "Epoch 112/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.9890\n",
      "Epoch 113/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0371 - accuracy: 0.9873\n",
      "Epoch 114/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0339 - accuracy: 0.9883\n",
      "Epoch 115/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.9920\n",
      "Epoch 116/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0418 - accuracy: 0.9877\n",
      "Epoch 117/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9803\n",
      "Epoch 118/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9820\n",
      "Epoch 119/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9817\n",
      "Epoch 120/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0402 - accuracy: 0.9883\n",
      "Epoch 121/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0424 - accuracy: 0.9870\n",
      "Epoch 122/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.9940\n",
      "Epoch 123/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9873\n",
      "Epoch 124/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9880\n",
      "Epoch 125/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9803\n",
      "Epoch 126/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9863\n",
      "Epoch 127/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0450 - accuracy: 0.9860\n",
      "Epoch 128/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9830\n",
      "Epoch 129/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.9863\n",
      "Epoch 130/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9920\n",
      "Epoch 131/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0329 - accuracy: 0.9903\n",
      "Epoch 132/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0395 - accuracy: 0.9897\n",
      "Epoch 133/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.9920\n",
      "Epoch 134/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9863\n",
      "Epoch 135/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0334 - accuracy: 0.9897\n",
      "Epoch 136/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0396 - accuracy: 0.9887\n",
      "Epoch 137/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9810\n",
      "Epoch 138/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9813\n",
      "Epoch 139/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0334 - accuracy: 0.9907\n",
      "Epoch 140/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9903\n",
      "Epoch 141/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.9867\n",
      "Epoch 142/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.9877\n",
      "Epoch 143/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.9900\n",
      "Epoch 144/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.9910\n",
      "Epoch 145/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.9910\n",
      "Epoch 146/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0378 - accuracy: 0.9900\n",
      "Epoch 147/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0340 - accuracy: 0.9877\n",
      "Epoch 148/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.9913\n",
      "Epoch 149/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0392 - accuracy: 0.9867\n",
      "Epoch 150/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9817\n",
      "Epoch 151/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9900\n",
      "Epoch 152/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0379 - accuracy: 0.9890\n",
      "Epoch 153/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 0.9877\n",
      "Epoch 154/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.9920\n",
      "Epoch 155/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.9930\n",
      "Epoch 156/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.9947\n",
      "Epoch 157/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.9913\n",
      "Epoch 158/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.9840\n",
      "Epoch 159/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.9893\n",
      "Epoch 160/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0420 - accuracy: 0.9877\n",
      "Epoch 161/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 0.9907\n",
      "Epoch 162/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0426 - accuracy: 0.9890\n",
      "Epoch 163/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.9907\n",
      "Epoch 164/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9880\n",
      "Epoch 165/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9903\n",
      "Epoch 166/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9887\n",
      "Epoch 167/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0320 - accuracy: 0.9907\n",
      "Epoch 168/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9873\n",
      "Epoch 169/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9850\n",
      "Epoch 170/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0430 - accuracy: 0.9863\n",
      "Epoch 171/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9853\n",
      "Epoch 172/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0431 - accuracy: 0.9867\n",
      "Epoch 173/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.9930\n",
      "Epoch 174/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9910\n",
      "Epoch 175/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.9943\n",
      "Epoch 176/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.9953\n",
      "Epoch 177/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.9930\n",
      "Epoch 178/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.9933\n",
      "Epoch 179/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 0.9883\n",
      "Epoch 180/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0429 - accuracy: 0.9873\n",
      "Epoch 181/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9867\n",
      "Epoch 182/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.9923\n",
      "Epoch 183/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.9937\n",
      "Epoch 184/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0362 - accuracy: 0.9923\n",
      "Epoch 185/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9957\n",
      "Epoch 186/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.9920\n",
      "Epoch 187/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0401 - accuracy: 0.9883\n",
      "Epoch 188/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0374 - accuracy: 0.9893\n",
      "Epoch 189/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.9927\n",
      "Epoch 190/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0391 - accuracy: 0.9887\n",
      "Epoch 191/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9923\n",
      "Epoch 192/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.9927\n",
      "Epoch 193/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.9923\n",
      "Epoch 194/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.9907\n",
      "Epoch 195/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0361 - accuracy: 0.9890\n",
      "Epoch 196/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.9880\n",
      "Epoch 197/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.9920\n",
      "Epoch 198/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.9950\n",
      "Epoch 199/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0403 - accuracy: 0.9870\n",
      "Epoch 200/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.9943\n",
      "Epoch 201/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.9947\n",
      "Epoch 202/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.9937\n",
      "Epoch 203/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.9893\n",
      "Epoch 204/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.9950\n",
      "Epoch 205/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.9927\n",
      "Epoch 206/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0355 - accuracy: 0.9893\n",
      "Epoch 207/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9837\n",
      "Epoch 208/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.9860\n",
      "Epoch 209/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0433 - accuracy: 0.9887\n",
      "Epoch 210/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.9903\n",
      "Epoch 211/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.9947\n",
      "Epoch 212/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.9950\n",
      "Epoch 213/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 0.9873\n",
      "Epoch 214/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9893\n",
      "Epoch 215/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9923\n",
      "Epoch 216/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.9923\n",
      "Epoch 217/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.9897\n",
      "Epoch 218/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0427 - accuracy: 0.9883\n",
      "Epoch 219/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9893\n",
      "Epoch 220/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.9937\n",
      "Epoch 221/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.9950\n",
      "Epoch 222/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.9937\n",
      "Epoch 223/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.9907\n",
      "Epoch 224/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.9930\n",
      "Epoch 225/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.9937\n",
      "Epoch 226/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9957\n",
      "Epoch 227/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.9950\n",
      "Epoch 228/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.9933\n",
      "Epoch 229/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.9927\n",
      "Epoch 230/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.9927\n",
      "Epoch 231/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9950\n",
      "Epoch 232/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.9950\n",
      "Epoch 233/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.9917\n",
      "Epoch 234/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 0.9873\n",
      "Epoch 235/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 0.9873\n",
      "Epoch 236/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.9903\n",
      "Epoch 237/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9863\n",
      "Epoch 238/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0355 - accuracy: 0.9893\n",
      "Epoch 239/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.9903\n",
      "Epoch 240/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9923\n",
      "Epoch 241/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.9963\n",
      "Epoch 242/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9973\n",
      "Epoch 243/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 0.9950\n",
      "Epoch 244/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0321 - accuracy: 0.9907\n",
      "Epoch 245/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.9917\n",
      "Epoch 246/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9947\n",
      "Epoch 247/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9970\n",
      "Epoch 248/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.9940\n",
      "Epoch 249/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.9947\n",
      "Epoch 250/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0169 - accuracy: 0.9933\n",
      "Epoch 251/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.9933\n",
      "Epoch 252/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9970\n",
      "Epoch 253/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.9913\n",
      "Epoch 254/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.9893\n",
      "Epoch 255/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.9920\n",
      "Epoch 256/256\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0150 - accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=dig_train['data'], y=dig_train['target'], \n",
    "                    epochs=model_sel.loc[best_index, 'epochs'],\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tovQW_Llc1Fw"
   },
   "source": [
    "## Test Model on Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1043,
     "status": "ok",
     "timestamp": 1677084326156,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -60
    },
    "id": "5g81CNxUc2Z8",
    "outputId": "f13ee303-f283-4e67-b664-477078fdcf8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step\n",
      "[[208   3   0   0   0   0   1   0   0   0]\n",
      " [  0 200   1   0   1   0   0   1   0   0]\n",
      " [  0   2 183   0   0   0   0   0   0   1]\n",
      " [  1   2   1 190   1   4   0   1   0   2]\n",
      " [  0   4   0   0 202   0   1   0   0   1]\n",
      " [  1   2   0   1   0 179   7   0   0   0]\n",
      " [  2   2   0   0   2   2 195   0   1   0]\n",
      " [  0   2   1   1   0   0   0 195   1   2]\n",
      " [  2   7   0   3   1   1   2   1 173   3]\n",
      " [  2  10   2   3   1   3   0   0   0 183]]\n",
      "\n",
      "\n",
      "Class 0:\n",
      "    Sensitivity (TPR):  98.113% (208 of 212)\n",
      "    Specificity (TNR):  99.554% (1784 of 1792)\n",
      "    Precision:          96.296% (208 of 216)\n",
      "    Neg. pred. value:   99.776% (1784 of 1788)\n",
      "Class 1:\n",
      "    Sensitivity (TPR):  98.522% (200 of 203)\n",
      "    Specificity (TNR):  98.112% (1767 of 1801)\n",
      "    Precision:          85.470% (200 of 234)\n",
      "    Neg. pred. value:   99.831% (1767 of 1770)\n",
      "Class 2:\n",
      "    Sensitivity (TPR):  98.387% (183 of 186)\n",
      "    Specificity (TNR):  99.725% (1813 of 1818)\n",
      "    Precision:          97.340% (183 of 188)\n",
      "    Neg. pred. value:   99.835% (1813 of 1816)\n",
      "Class 3:\n",
      "    Sensitivity (TPR):  94.059% (190 of 202)\n",
      "    Specificity (TNR):  99.556% (1794 of 1802)\n",
      "    Precision:          95.960% (190 of 198)\n",
      "    Neg. pred. value:   99.336% (1794 of 1806)\n",
      "Class 4:\n",
      "    Sensitivity (TPR):  97.115% (202 of 208)\n",
      "    Specificity (TNR):  99.666% (1790 of 1796)\n",
      "    Precision:          97.115% (202 of 208)\n",
      "    Neg. pred. value:   99.666% (1790 of 1796)\n",
      "Class 5:\n",
      "    Sensitivity (TPR):  94.211% (179 of 190)\n",
      "    Specificity (TNR):  99.449% (1804 of 1814)\n",
      "    Precision:          94.709% (179 of 189)\n",
      "    Neg. pred. value:   99.394% (1804 of 1815)\n",
      "Class 6:\n",
      "    Sensitivity (TPR):  95.588% (195 of 204)\n",
      "    Specificity (TNR):  99.389% (1789 of 1800)\n",
      "    Precision:          94.660% (195 of 206)\n",
      "    Neg. pred. value:   99.499% (1789 of 1798)\n",
      "Class 7:\n",
      "    Sensitivity (TPR):  96.535% (195 of 202)\n",
      "    Specificity (TNR):  99.834% (1799 of 1802)\n",
      "    Precision:          98.485% (195 of 198)\n",
      "    Neg. pred. value:   99.612% (1799 of 1806)\n",
      "Class 8:\n",
      "    Sensitivity (TPR):  89.637% (173 of 193)\n",
      "    Specificity (TNR):  99.890% (1809 of 1811)\n",
      "    Precision:          98.857% (173 of 175)\n",
      "    Neg. pred. value:   98.907% (1809 of 1829)\n",
      "Class 9:\n",
      "    Sensitivity (TPR):  89.706% (183 of 204)\n",
      "    Specificity (TNR):  99.500% (1791 of 1800)\n",
      "    Precision:          95.312% (183 of 192)\n",
      "    Neg. pred. value:   98.841% (1791 of 1812)\n",
      "\n",
      "Overall accuracy:   95.210% (1908 of 2004)\n",
      "Balanced accuracy:  95.187%\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(dig_test['data'])\n",
    "\n",
    "evaluate_classification_result(dig_test['target'], pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Pritz_Sebastian.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python (MLTeaching)",
   "language": "python",
   "name": "mlteaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
