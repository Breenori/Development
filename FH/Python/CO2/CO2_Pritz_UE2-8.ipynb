{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "606q4MQJZz2U"
      },
      "source": [
        "# Model Selection for Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp1ELm57Zz2a"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "executionInfo": {
          "elapsed": 424,
          "status": "ok",
          "timestamp": 1677083511919,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "HDmggAMy91S3"
      },
      "outputs": [],
      "source": [
        "## adapt this directory to your needs\n",
        "data_dir = 'data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "executionInfo": {
          "elapsed": 9207,
          "status": "ok",
          "timestamp": 1677083550034,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "vXCty5i0Zz2b"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from pyMLaux import show_img_data, plot_history, evaluate_classification_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1108,
          "status": "ok",
          "timestamp": 1677083558884,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "WvZsf7gkPSOY",
        "outputId": "c2fdd292-2aa3-4465-d295-4ca271aff53c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c9KhDatZz2o"
      },
      "source": [
        "## Load Simple Digit Recognition Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "executionInfo": {
          "elapsed": 1877,
          "status": "ok",
          "timestamp": 1677083565384,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "D6TPTi1qZz2o"
      },
      "outputs": [],
      "source": [
        "dig_train_raw = pd.read_csv(data_dir + 'Digits_training.csv', sep=',')\n",
        "dig_train = {'data': np.array(dig_train_raw.iloc[:, :-1]),\n",
        "             'target': np.array(dig_train_raw.iloc[:, -1]),\n",
        "             'feature_names': dig_train_raw.columns[:-1],\n",
        "             'target_names': [str(i) for i in range(0, 10)]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "executionInfo": {
          "elapsed": 1414,
          "status": "ok",
          "timestamp": 1677083569578,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "NMbX-NupZz2p"
      },
      "outputs": [],
      "source": [
        "dig_test_raw = pd.read_csv(data_dir + 'Digits_test.csv', sep=',')\n",
        "dig_test = {'data': np.array(dig_test_raw.iloc[:, :-1]),\n",
        "            'target': np.array(dig_test_raw.iloc[:, -1]),\n",
        "            'feature_names': dig_test_raw.columns[:-1],\n",
        "            'target_names': [str(i) for i in range(0, 10)]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "executionInfo": {
          "elapsed": 1741,
          "status": "ok",
          "timestamp": 1677083576577,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "Hdd_vh4uZz2p",
        "outputId": "8467f971-5d75-4088-c298-01104ace1c21"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 400x400 with 30 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_img_data(dig_train['data'].reshape((dig_train['data'].shape[0], 6, 4, 1)), figsize=(4, 4),\n",
        "              interpolation=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1677083579167,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "IUlRxKWRZz2p",
        "outputId": "9542b6aa-e39b-4729-c0b7-903712feed92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[8, 5, 1, 0, 6, 0],\n",
              "       [3, 2, 3, 4, 9, 5],\n",
              "       [3, 3, 1, 6, 4, 7],\n",
              "       [5, 0, 1, 0, 2, 0],\n",
              "       [2, 6, 5, 2, 7, 2]], dtype=int64)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dig_train['target'][range(0, 30)].reshape(5, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1677083583460,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "lzYOx6rycIdF"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(dig_train['data'], dig_train['target'],\n",
        "                                                  test_size=0.3, random_state=4232)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIRr1M4iZz2r"
      },
      "source": [
        "## Functions for Hyperparameter/Architecture Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this method, I decided to pass the options as parameters, so I can change them on the fly for the versions of 100 variations and 50 variations later.\n",
        "I also decided to enlarge the range of the learning rate because the best models previously often shared the lower bound. I mistakenly also changed the upper bound, but didn't want to execute it again since it would cause different results yet again. I also settled to sort the \"Hidden Layer\" list, to avoid fluctuating numbers of neurons per layer (avoid going up and down with the count). Finally, sigmoid was removed from the list of activation functions, due to the vanishing gradient problem in deep neural networks. Other changes from the base file include changing some of the vectors slightly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "executionInfo": {
          "elapsed": 420,
          "status": "ok",
          "timestamp": 1677083593130,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "YJd7EuX0bLSG"
      },
      "outputs": [],
      "source": [
        "def create_hyperparams(n, \n",
        "                       lr, \n",
        "                       epoch_options, \n",
        "                       no_layers_options, \n",
        "                       hidden_layer_options,\n",
        "                       dropout_options, \n",
        "                       activation_options):\n",
        "    df = pd.DataFrame(index=range(n),\n",
        "                      columns=['no_hidden_layers', 'hidden_layers', 'activation', 'dropout', 'lr', 'epochs'])\n",
        "\n",
        "    for i in range(n):\n",
        "        df.loc[i, 'lr'] = lr * 5.**random.uniform(-2., 2.)\n",
        "        df.loc[i, 'epochs'] = random.sample(epoch_options, 1)[0]\n",
        "    \n",
        "        no_layers = random.randint(1, 4)\n",
        "        df.loc[i, 'no_hidden_layers'] = no_layers \n",
        "        df.loc[i, 'hidden_layers'] = sorted([int(random.sample(hidden_layer_options, 1)[0]) for i in range(no_layers)], reverse=True)\n",
        "        df.loc[i, 'dropout'] = random.sample(dropout_options, 1)[0]\n",
        "        df.loc[i, 'activation'] = random.sample(activation_options, 1)[0]\n",
        "    \n",
        "    return(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "executionInfo": {
          "elapsed": 416,
          "status": "ok",
          "timestamp": 1677083606711,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "fFtzoyAFbaTf"
      },
      "outputs": [],
      "source": [
        "def create_network(hp, no_inputs, no_outputs, output_activation='softmax', **kwargs):\n",
        "    hidden_layers = hp['hidden_layers']\n",
        "    \n",
        "    dropout = hp['dropout']\n",
        "    hidden_activation = hp['activation']\n",
        "    \n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(no_inputs, )))\n",
        "\n",
        "    for cl in hidden_layers:\n",
        "        model.add(tf.keras.layers.Dense(cl, activation=hidden_activation))\n",
        "        if dropout > 0:\n",
        "            model.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "    model.add(tf.keras.layers.Dense(no_outputs, activation=output_activation)) \n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=hp['lr'])\n",
        "\n",
        "    model.compile(optimizer=opt, **kwargs)\n",
        "\n",
        "    return(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1677083644625,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "KxQzPx_-bpl9"
      },
      "outputs": [],
      "source": [
        "def find_best(df, crit='ACC'):\n",
        "    index = np.where(df[crit] == np.amax(df[crit]))[0]\n",
        "    return(df.iloc[list(index), :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2zsFjuDbrDW"
      },
      "source": [
        "## Perform Model Selection and Determine Best Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set of 100 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "executionInfo": {
          "elapsed": 423,
          "status": "ok",
          "timestamp": 1677083653798,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "D0tU9QQKbvb8"
      },
      "outputs": [],
      "source": [
        "random.seed(4232)\n",
        "batch_size = 32\n",
        "no_models = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1677083659647,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "0_CsQAjUbzws"
      },
      "outputs": [],
      "source": [
        "model_sel = create_hyperparams(no_models, \n",
        "                               0.001, \n",
        "                               [32, 64, 128, 256], \n",
        "                               [1, 2, 3, 4], \n",
        "                               [64, 256, 512],\n",
        "                               [0.2, 0.25, 0.3], \n",
        "                               ['relu', 'elu'])\n",
        "model_sel['ACC'] = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "executionInfo": {
          "elapsed": 16,
          "status": "ok",
          "timestamp": 1677083662728,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "W7CmdoUuzMvD",
        "outputId": "2140bc42-35c8-4795-f611-76b1845c9f18"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no_hidden_layers</th>\n",
              "      <th>hidden_layers</th>\n",
              "      <th>activation</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>epochs</th>\n",
              "      <th>ACC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 64, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 64, 64, 64]</td>\n",
              "      <td>elu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.006199</td>\n",
              "      <td>32</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 256, 256]</td>\n",
              "      <td>elu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.003647</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 256, 64]</td>\n",
              "      <td>elu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.016225</td>\n",
              "      <td>32</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.024626</td>\n",
              "      <td>32</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>elu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>64</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>3</td>\n",
              "      <td>[256, 256, 64]</td>\n",
              "      <td>elu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000404</td>\n",
              "      <td>32</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000766</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 512, 256, 256]</td>\n",
              "      <td>elu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.001503</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>2</td>\n",
              "      <td>[256, 64]</td>\n",
              "      <td>elu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>32</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   no_hidden_layers         hidden_layers activation dropout        lr epochs  \\\n",
              "0                 4     [512, 64, 64, 64]       relu    0.25  0.000714    128   \n",
              "1                 4     [512, 64, 64, 64]        elu     0.3  0.006199     32   \n",
              "2                 4  [512, 256, 256, 256]        elu     0.3  0.003647    256   \n",
              "3                 4   [512, 256, 256, 64]        elu     0.3  0.016225     32   \n",
              "4                 2            [512, 512]       relu     0.2  0.024626     32   \n",
              "..              ...                   ...        ...     ...       ...    ...   \n",
              "95                2            [512, 256]        elu     0.2  0.000108     64   \n",
              "96                3        [256, 256, 64]        elu     0.3  0.000404     32   \n",
              "97                3         [512, 64, 64]       relu    0.25  0.000766    256   \n",
              "98                4  [512, 512, 256, 256]        elu    0.25  0.001503    256   \n",
              "99                2             [256, 64]        elu     0.3  0.003744     32   \n",
              "\n",
              "    ACC  \n",
              "0    -1  \n",
              "1    -1  \n",
              "2    -1  \n",
              "3    -1  \n",
              "4    -1  \n",
              "..  ...  \n",
              "95   -1  \n",
              "96   -1  \n",
              "97   -1  \n",
              "98   -1  \n",
              "99   -1  \n",
              "\n",
              "[100 rows x 7 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_sel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "96f90d35de194292a9aac81de1a44fb7",
            "6e1925a876534f50bb242cbcf0940ab6",
            "dfc48cbf68cb4294951b0d48e7d0242c",
            "16acc394f36d4f62bbc116ff572e5bfb",
            "55901904beb64afab291086ae9b38327",
            "ebb9c2065ba341f794d33c5583f12010",
            "15bf4c1be6fe44928d201f1dea9d485f",
            "be98efb613024054adea69a32b6e6e3a",
            "3f541caf30464caba86dc1d31c50a736",
            "d19875237b2e4337abed07843571303e",
            "5272552405754f1f85a290c8f65885fe"
          ]
        },
        "executionInfo": {
          "elapsed": 449814,
          "status": "ok",
          "timestamp": 1677084214029,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "mPYnYCYKb2Ug",
        "outputId": "c5c4793c-f19e-4604-a181-ae23c8a06100"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46f7a81690c64f3ab5976877b22770dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for i in tqdm(range(no_models)):\n",
        "    model = create_network(model_sel.iloc[i], no_inputs=X_train.shape[1],\n",
        "                           no_outputs=10, loss='sparse_categorical_crossentropy', \n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(x=X_train, y=y_train, \n",
        "                        epochs=model_sel['epochs'][i],\n",
        "                        batch_size=batch_size, \n",
        "                        verbose=0)\n",
        "\n",
        "    pred = model.predict(x=X_val, verbose=0)\n",
        "    predC = np.argmax(pred, axis=1)\n",
        "\n",
        "    model_sel.loc[i, 'ACC'] = accuracy_score(y_val, predC)\n",
        "\n",
        "    tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "executionInfo": {
          "elapsed": 1419,
          "status": "ok",
          "timestamp": 1677084270815,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "jo0QXrbPnK_N",
        "outputId": "e1e5b975-981a-4129-ef61-54009efc196b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no_hidden_layers</th>\n",
              "      <th>hidden_layers</th>\n",
              "      <th>activation</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>epochs</th>\n",
              "      <th>ACC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>256</td>\n",
              "      <td>0.955556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000766</td>\n",
              "      <td>256</td>\n",
              "      <td>0.955556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 512, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000484</td>\n",
              "      <td>64</td>\n",
              "      <td>0.953333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>1</td>\n",
              "      <td>[512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.009536</td>\n",
              "      <td>256</td>\n",
              "      <td>0.953333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000919</td>\n",
              "      <td>256</td>\n",
              "      <td>0.953333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 64, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>128</td>\n",
              "      <td>0.952222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.002918</td>\n",
              "      <td>64</td>\n",
              "      <td>0.952222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 512, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>64</td>\n",
              "      <td>0.952222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000604</td>\n",
              "      <td>256</td>\n",
              "      <td>0.951111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 512, 512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>64</td>\n",
              "      <td>0.950000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   no_hidden_layers        hidden_layers activation dropout        lr epochs  \\\n",
              "38                4  [512, 256, 256, 64]       relu    0.25  0.000113    256   \n",
              "97                3        [512, 64, 64]       relu    0.25  0.000766    256   \n",
              "75                4  [512, 512, 256, 64]       relu     0.3  0.000484     64   \n",
              "52                1                [512]       relu     0.3  0.009536    256   \n",
              "80                2            [512, 64]       relu    0.25  0.000919    256   \n",
              "0                 4    [512, 64, 64, 64]       relu    0.25  0.000714    128   \n",
              "37                2            [512, 64]       relu     0.2  0.002918     64   \n",
              "6                 4  [512, 512, 256, 64]       relu    0.25  0.000462     64   \n",
              "56                3       [512, 512, 64]       relu     0.2  0.000604    256   \n",
              "71                3      [512, 512, 512]       relu     0.3  0.000148     64   \n",
              "\n",
              "         ACC  \n",
              "38  0.955556  \n",
              "97  0.955556  \n",
              "75  0.953333  \n",
              "52  0.953333  \n",
              "80  0.953333  \n",
              "0   0.952222  \n",
              "37  0.952222  \n",
              "6   0.952222  \n",
              "56  0.951111  \n",
              "71  0.950000  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_sel.sort_values(by='ACC', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "executionInfo": {
          "elapsed": 433,
          "status": "ok",
          "timestamp": 1677084278492,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "oKRrMA27ciRn",
        "outputId": "331879d1-5c7e-4db6-e67c-d857477c0723"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no_hidden_layers</th>\n",
              "      <th>hidden_layers</th>\n",
              "      <th>activation</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>epochs</th>\n",
              "      <th>ACC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>256</td>\n",
              "      <td>0.955556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000766</td>\n",
              "      <td>256</td>\n",
              "      <td>0.955556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   no_hidden_layers        hidden_layers activation dropout        lr epochs  \\\n",
              "38                4  [512, 256, 256, 64]       relu    0.25  0.000113    256   \n",
              "97                3        [512, 64, 64]       relu    0.25  0.000766    256   \n",
              "\n",
              "         ACC  \n",
              "38  0.955556  \n",
              "97  0.955556  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_best(model_sel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1677084282946,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "W7C3g4pHcG67"
      },
      "outputs": [],
      "source": [
        "best_index = find_best(model_sel).index[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Narrow down to 50\n",
        "The best 10 models before, only rarely featured only 1 hidden layer, and only used RELU as an activation function. Also, 32 Epochs were never used. I'd therefor remove these options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "no_models = 50\n",
        "model_sel = create_hyperparams(no_models, \n",
        "                               0.001, \n",
        "                               [128, 256], \n",
        "                               [2, 3, 4], \n",
        "                               [64, 256, 512],\n",
        "                               [0.2, 0.25, 0.3], \n",
        "                               ['relu'])\n",
        "model_sel['ACC'] = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no_hidden_layers</th>\n",
              "      <th>hidden_layers</th>\n",
              "      <th>activation</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>epochs</th>\n",
              "      <th>ACC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[256, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>[512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00168</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.004181</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.021843</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 256, 256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>[64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000696</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000796</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>[512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3</td>\n",
              "      <td>[256, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.008993</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 512, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>[256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>[512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.002774</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 512, 512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 512, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005072</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 512, 512, 256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2</td>\n",
              "      <td>[256, 256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.003635</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.006097</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2</td>\n",
              "      <td>[256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.005553</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3</td>\n",
              "      <td>[256, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.006418</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 512, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.003603</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.018461</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.021523</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>[512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.004529</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000746</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1</td>\n",
              "      <td>[256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000946</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 256, 256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000446</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.004146</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000842</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1</td>\n",
              "      <td>[64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 512, 512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.002316</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000441</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1</td>\n",
              "      <td>[512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000781</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1</td>\n",
              "      <td>[256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.002875</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 256, 256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.00085</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1</td>\n",
              "      <td>[512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001038</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 512, 256, 256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.00011</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.007209</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.007321</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>3</td>\n",
              "      <td>[256, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.01341</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000863</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.003552</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 512, 256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.022258</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>2</td>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.016378</td>\n",
              "      <td>128</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   no_hidden_layers         hidden_layers activation dropout        lr epochs  \\\n",
              "0                 1                 [256]       relu     0.3  0.000331    256   \n",
              "1                 4    [512, 256, 64, 64]       relu     0.3  0.000416    256   \n",
              "2                 2              [64, 64]       relu    0.25  0.000056    256   \n",
              "3                 3        [256, 256, 64]       relu     0.2  0.000277    128   \n",
              "4                 1                 [512]       relu     0.2   0.00168    256   \n",
              "5                 3         [512, 64, 64]       relu    0.25  0.004181    256   \n",
              "6                 2             [512, 64]       relu    0.25  0.021843    256   \n",
              "7                 3       [512, 256, 256]       relu    0.25  0.000103    256   \n",
              "8                 2             [512, 64]       relu    0.25  0.000719    256   \n",
              "9                 1                  [64]       relu     0.2  0.000696    256   \n",
              "10                4    [512, 256, 64, 64]       relu    0.25  0.000796    256   \n",
              "11                4    [512, 256, 64, 64]       relu     0.3  0.000866    256   \n",
              "12                1                 [512]       relu     0.2  0.000081    128   \n",
              "13                3        [256, 256, 64]       relu    0.25  0.008993    128   \n",
              "14                4    [512, 512, 64, 64]       relu     0.2  0.000172    128   \n",
              "15                1                 [256]       relu     0.3  0.000121    256   \n",
              "16                1                 [512]       relu     0.2  0.002774    128   \n",
              "17                4   [512, 512, 512, 64]       relu    0.25  0.000068    128   \n",
              "18                4   [512, 512, 256, 64]       relu     0.2  0.005072    256   \n",
              "19                4  [512, 512, 512, 256]       relu     0.2  0.000126    128   \n",
              "20                2            [256, 256]       relu     0.3  0.003635    256   \n",
              "21                3        [512, 256, 64]       relu     0.3  0.006097    256   \n",
              "22                2             [256, 64]       relu     0.3  0.005553    256   \n",
              "23                3        [256, 256, 64]       relu     0.2  0.000205    256   \n",
              "24                3        [512, 512, 64]       relu    0.25  0.006418    128   \n",
              "25                4    [512, 512, 64, 64]       relu     0.3  0.003603    128   \n",
              "26                4    [512, 256, 64, 64]       relu     0.2  0.000047    256   \n",
              "27                3        [512, 512, 64]       relu    0.25  0.018461    256   \n",
              "28                3         [512, 64, 64]       relu     0.2  0.021523    256   \n",
              "29                1                 [512]       relu    0.25  0.004529    128   \n",
              "30                4   [512, 256, 256, 64]       relu    0.25  0.000746    256   \n",
              "31                1                 [256]       relu     0.3  0.000946    256   \n",
              "32                4  [512, 256, 256, 256]       relu     0.3  0.000446    256   \n",
              "33                3         [512, 64, 64]       relu     0.2  0.004146    128   \n",
              "34                2            [512, 256]       relu     0.3  0.000842    256   \n",
              "35                1                  [64]       relu    0.25  0.000048    256   \n",
              "36                4   [512, 512, 512, 64]       relu     0.2  0.002316    256   \n",
              "37                2             [512, 64]       relu     0.3  0.000441    128   \n",
              "38                1                 [512]       relu     0.2  0.000781    256   \n",
              "39                1                 [256]       relu    0.25  0.002875    256   \n",
              "40                3       [512, 256, 256]       relu     0.3   0.00085    128   \n",
              "41                1                 [512]       relu     0.2  0.001038    256   \n",
              "42                4  [512, 512, 256, 256]       relu     0.3   0.00011    128   \n",
              "43                3        [512, 256, 64]       relu     0.3  0.007209    128   \n",
              "44                3         [512, 64, 64]       relu     0.3  0.007321    128   \n",
              "45                3         [256, 64, 64]       relu    0.25   0.01341    128   \n",
              "46                3        [512, 256, 64]       relu     0.2  0.000863    256   \n",
              "47                3         [512, 64, 64]       relu     0.3  0.003552    256   \n",
              "48                3       [512, 512, 256]       relu     0.3  0.022258    256   \n",
              "49                2              [64, 64]       relu     0.2  0.016378    128   \n",
              "\n",
              "    ACC  \n",
              "0    -1  \n",
              "1    -1  \n",
              "2    -1  \n",
              "3    -1  \n",
              "4    -1  \n",
              "5    -1  \n",
              "6    -1  \n",
              "7    -1  \n",
              "8    -1  \n",
              "9    -1  \n",
              "10   -1  \n",
              "11   -1  \n",
              "12   -1  \n",
              "13   -1  \n",
              "14   -1  \n",
              "15   -1  \n",
              "16   -1  \n",
              "17   -1  \n",
              "18   -1  \n",
              "19   -1  \n",
              "20   -1  \n",
              "21   -1  \n",
              "22   -1  \n",
              "23   -1  \n",
              "24   -1  \n",
              "25   -1  \n",
              "26   -1  \n",
              "27   -1  \n",
              "28   -1  \n",
              "29   -1  \n",
              "30   -1  \n",
              "31   -1  \n",
              "32   -1  \n",
              "33   -1  \n",
              "34   -1  \n",
              "35   -1  \n",
              "36   -1  \n",
              "37   -1  \n",
              "38   -1  \n",
              "39   -1  \n",
              "40   -1  \n",
              "41   -1  \n",
              "42   -1  \n",
              "43   -1  \n",
              "44   -1  \n",
              "45   -1  \n",
              "46   -1  \n",
              "47   -1  \n",
              "48   -1  \n",
              "49   -1  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_sel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9d29f0d5ca4422a86c78abefa6156be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for i in tqdm(range(no_models)):\n",
        "    model = create_network(model_sel.iloc[i], no_inputs=X_train.shape[1],\n",
        "                           no_outputs=10, loss='sparse_categorical_crossentropy', \n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(x=X_train, y=y_train, \n",
        "                        epochs=model_sel['epochs'][i],\n",
        "                        batch_size=batch_size, \n",
        "                        verbose=0)\n",
        "\n",
        "    pred = model.predict(x=X_val, verbose=0)\n",
        "    predC = np.argmax(pred, axis=1)\n",
        "\n",
        "    model_sel.loc[i, 'ACC'] = accuracy_score(y_val, predC)\n",
        "\n",
        "    tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no_hidden_layers</th>\n",
              "      <th>hidden_layers</th>\n",
              "      <th>activation</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>epochs</th>\n",
              "      <th>ACC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000796</td>\n",
              "      <td>256</td>\n",
              "      <td>0.957778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000441</td>\n",
              "      <td>128</td>\n",
              "      <td>0.956667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>256</td>\n",
              "      <td>0.956667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1</td>\n",
              "      <td>[512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000781</td>\n",
              "      <td>256</td>\n",
              "      <td>0.956667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>256</td>\n",
              "      <td>0.956667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>3</td>\n",
              "      <td>[512, 256, 256]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.00085</td>\n",
              "      <td>128</td>\n",
              "      <td>0.956667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3</td>\n",
              "      <td>[256, 256, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>256</td>\n",
              "      <td>0.955556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>[512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00168</td>\n",
              "      <td>256</td>\n",
              "      <td>0.955556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1</td>\n",
              "      <td>[512]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001038</td>\n",
              "      <td>256</td>\n",
              "      <td>0.954444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 512, 512, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>128</td>\n",
              "      <td>0.953333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   no_hidden_layers        hidden_layers activation dropout        lr epochs  \\\n",
              "10                4   [512, 256, 64, 64]       relu    0.25  0.000796    256   \n",
              "37                2            [512, 64]       relu     0.3  0.000441    128   \n",
              "8                 2            [512, 64]       relu    0.25  0.000719    256   \n",
              "38                1                [512]       relu     0.2  0.000781    256   \n",
              "1                 4   [512, 256, 64, 64]       relu     0.3  0.000416    256   \n",
              "40                3      [512, 256, 256]       relu     0.3   0.00085    128   \n",
              "23                3       [256, 256, 64]       relu     0.2  0.000205    256   \n",
              "4                 1                [512]       relu     0.2   0.00168    256   \n",
              "41                1                [512]       relu     0.2  0.001038    256   \n",
              "17                4  [512, 512, 512, 64]       relu    0.25  0.000068    128   \n",
              "\n",
              "         ACC  \n",
              "10  0.957778  \n",
              "37  0.956667  \n",
              "8   0.956667  \n",
              "38  0.956667  \n",
              "1   0.956667  \n",
              "40  0.956667  \n",
              "23  0.955556  \n",
              "4   0.955556  \n",
              "41  0.954444  \n",
              "17  0.953333  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_sel.sort_values(by='ACC', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no_hidden_layers</th>\n",
              "      <th>hidden_layers</th>\n",
              "      <th>activation</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>epochs</th>\n",
              "      <th>ACC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>[512, 256, 64, 64]</td>\n",
              "      <td>relu</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000796</td>\n",
              "      <td>256</td>\n",
              "      <td>0.957778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   no_hidden_layers       hidden_layers activation dropout        lr epochs  \\\n",
              "10                4  [512, 256, 64, 64]       relu    0.25  0.000796    256   \n",
              "\n",
              "         ACC  \n",
              "10  0.957778  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_best(model_sel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_index = find_best(model_sel).index[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3gGoxx4cldW"
      },
      "source": [
        "## Train Model on Entire Training Set Using Best Parameters \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "executionInfo": {
          "elapsed": 452,
          "status": "ok",
          "timestamp": 1677084292891,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "dHrP42SjcoGa"
      },
      "outputs": [],
      "source": [
        "model = create_network(model_sel.loc[best_index], no_inputs=X_train.shape[1],\n",
        "                       no_outputs=10, loss='sparse_categorical_crossentropy', \n",
        "                       metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 7,
          "status": "ok",
          "timestamp": 1677084297286,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "GevKGfVpcqUe",
        "outputId": "361987f5-55eb-4294-e72c-5035b8e1430e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               12800     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 165,386\n",
            "Trainable params: 165,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 11978,
          "status": "ok",
          "timestamp": 1677084316373,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "8SOr03A1ctap",
        "outputId": "a7421921-142e-4f64-f945-96c40c96478b",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/256\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 1.9176 - accuracy: 0.3143\n",
            "Epoch 2/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 1.0956 - accuracy: 0.6317\n",
            "Epoch 3/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.7650 - accuracy: 0.7573\n",
            "Epoch 4/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.6489 - accuracy: 0.8027\n",
            "Epoch 5/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.8347\n",
            "Epoch 6/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.4705 - accuracy: 0.8573\n",
            "Epoch 7/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.4153 - accuracy: 0.8717\n",
            "Epoch 8/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.8830\n",
            "Epoch 9/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.3556 - accuracy: 0.8897\n",
            "Epoch 10/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.3362 - accuracy: 0.8993\n",
            "Epoch 11/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.3142 - accuracy: 0.9070\n",
            "Epoch 12/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2753 - accuracy: 0.9200\n",
            "Epoch 13/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.9140\n",
            "Epoch 14/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2662 - accuracy: 0.9233\n",
            "Epoch 15/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.9240\n",
            "Epoch 16/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.9240\n",
            "Epoch 17/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2520 - accuracy: 0.9260\n",
            "Epoch 18/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9373\n",
            "Epoch 19/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2115 - accuracy: 0.9360\n",
            "Epoch 20/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1999 - accuracy: 0.9433\n",
            "Epoch 21/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1938 - accuracy: 0.9403\n",
            "Epoch 22/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1791 - accuracy: 0.9447\n",
            "Epoch 23/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.9403\n",
            "Epoch 24/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1769 - accuracy: 0.9457\n",
            "Epoch 25/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1609 - accuracy: 0.9523\n",
            "Epoch 26/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9520\n",
            "Epoch 27/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1506 - accuracy: 0.9520\n",
            "Epoch 28/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1580 - accuracy: 0.9503\n",
            "Epoch 29/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9480\n",
            "Epoch 30/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1342 - accuracy: 0.9597\n",
            "Epoch 31/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1409 - accuracy: 0.9563\n",
            "Epoch 32/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1331 - accuracy: 0.9627\n",
            "Epoch 33/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1527 - accuracy: 0.9533\n",
            "Epoch 34/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9517\n",
            "Epoch 35/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.9620\n",
            "Epoch 36/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1320 - accuracy: 0.9573\n",
            "Epoch 37/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1295 - accuracy: 0.9590\n",
            "Epoch 38/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1117 - accuracy: 0.9680\n",
            "Epoch 39/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1052 - accuracy: 0.9683\n",
            "Epoch 40/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1005 - accuracy: 0.9683\n",
            "Epoch 41/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1130 - accuracy: 0.9630\n",
            "Epoch 42/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1004 - accuracy: 0.9693\n",
            "Epoch 43/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0850 - accuracy: 0.9743\n",
            "Epoch 44/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1190 - accuracy: 0.9603\n",
            "Epoch 45/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0994 - accuracy: 0.9697\n",
            "Epoch 46/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.9690\n",
            "Epoch 47/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1062 - accuracy: 0.9683\n",
            "Epoch 48/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0917 - accuracy: 0.9747\n",
            "Epoch 49/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0974 - accuracy: 0.9683\n",
            "Epoch 50/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9703\n",
            "Epoch 51/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0845 - accuracy: 0.9747\n",
            "Epoch 52/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0920 - accuracy: 0.9737\n",
            "Epoch 53/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9820\n",
            "Epoch 54/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0861 - accuracy: 0.9743\n",
            "Epoch 55/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9723\n",
            "Epoch 56/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0886 - accuracy: 0.9737\n",
            "Epoch 57/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.9767\n",
            "Epoch 58/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0957 - accuracy: 0.9700\n",
            "Epoch 59/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.9733\n",
            "Epoch 60/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.9770\n",
            "Epoch 61/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9780\n",
            "Epoch 62/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9740\n",
            "Epoch 63/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9767\n",
            "Epoch 64/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9823\n",
            "Epoch 65/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9787\n",
            "Epoch 66/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0784 - accuracy: 0.9747\n",
            "Epoch 67/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1032 - accuracy: 0.9693\n",
            "Epoch 68/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.9790\n",
            "Epoch 69/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9820\n",
            "Epoch 70/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9817\n",
            "Epoch 71/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.9787\n",
            "Epoch 72/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9870\n",
            "Epoch 73/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9810\n",
            "Epoch 74/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9803\n",
            "Epoch 75/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9860\n",
            "Epoch 76/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9773\n",
            "Epoch 77/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9827\n",
            "Epoch 78/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9803\n",
            "Epoch 79/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.9757\n",
            "Epoch 80/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9780\n",
            "Epoch 81/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9817\n",
            "Epoch 82/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9843\n",
            "Epoch 83/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9810\n",
            "Epoch 84/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9787\n",
            "Epoch 85/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9800\n",
            "Epoch 86/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9823\n",
            "Epoch 87/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9813\n",
            "Epoch 88/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.9773\n",
            "Epoch 89/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9863\n",
            "Epoch 90/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9793\n",
            "Epoch 91/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9817\n",
            "Epoch 92/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9843\n",
            "Epoch 93/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9860\n",
            "Epoch 94/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0403 - accuracy: 0.9870\n",
            "Epoch 95/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 0.9893\n",
            "Epoch 96/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9847\n",
            "Epoch 97/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0400 - accuracy: 0.9873\n",
            "Epoch 98/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9810\n",
            "Epoch 99/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0431 - accuracy: 0.9847\n",
            "Epoch 100/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0424 - accuracy: 0.9853\n",
            "Epoch 101/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9867\n",
            "Epoch 102/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9847\n",
            "Epoch 103/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9870\n",
            "Epoch 104/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9843\n",
            "Epoch 105/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9857\n",
            "Epoch 106/256\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 0.9850\n",
            "Epoch 107/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9827\n",
            "Epoch 108/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9810\n",
            "Epoch 109/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9893\n",
            "Epoch 110/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9813\n",
            "Epoch 111/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.9907\n",
            "Epoch 112/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.9890\n",
            "Epoch 113/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0371 - accuracy: 0.9873\n",
            "Epoch 114/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0339 - accuracy: 0.9883\n",
            "Epoch 115/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.9920\n",
            "Epoch 116/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0418 - accuracy: 0.9877\n",
            "Epoch 117/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9803\n",
            "Epoch 118/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9820\n",
            "Epoch 119/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9817\n",
            "Epoch 120/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0402 - accuracy: 0.9883\n",
            "Epoch 121/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0424 - accuracy: 0.9870\n",
            "Epoch 122/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.9940\n",
            "Epoch 123/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9873\n",
            "Epoch 124/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9880\n",
            "Epoch 125/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9803\n",
            "Epoch 126/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9863\n",
            "Epoch 127/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0450 - accuracy: 0.9860\n",
            "Epoch 128/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9830\n",
            "Epoch 129/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.9863\n",
            "Epoch 130/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9920\n",
            "Epoch 131/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0329 - accuracy: 0.9903\n",
            "Epoch 132/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0395 - accuracy: 0.9897\n",
            "Epoch 133/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.9920\n",
            "Epoch 134/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9863\n",
            "Epoch 135/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0334 - accuracy: 0.9897\n",
            "Epoch 136/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0396 - accuracy: 0.9887\n",
            "Epoch 137/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9810\n",
            "Epoch 138/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9813\n",
            "Epoch 139/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0334 - accuracy: 0.9907\n",
            "Epoch 140/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9903\n",
            "Epoch 141/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.9867\n",
            "Epoch 142/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.9877\n",
            "Epoch 143/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.9900\n",
            "Epoch 144/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.9910\n",
            "Epoch 145/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.9910\n",
            "Epoch 146/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0378 - accuracy: 0.9900\n",
            "Epoch 147/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0340 - accuracy: 0.9877\n",
            "Epoch 148/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.9913\n",
            "Epoch 149/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0392 - accuracy: 0.9867\n",
            "Epoch 150/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9817\n",
            "Epoch 151/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9900\n",
            "Epoch 152/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0379 - accuracy: 0.9890\n",
            "Epoch 153/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 0.9877\n",
            "Epoch 154/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.9920\n",
            "Epoch 155/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.9930\n",
            "Epoch 156/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.9947\n",
            "Epoch 157/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.9913\n",
            "Epoch 158/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.9840\n",
            "Epoch 159/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.9893\n",
            "Epoch 160/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0420 - accuracy: 0.9877\n",
            "Epoch 161/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 0.9907\n",
            "Epoch 162/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0426 - accuracy: 0.9890\n",
            "Epoch 163/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.9907\n",
            "Epoch 164/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9880\n",
            "Epoch 165/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9903\n",
            "Epoch 166/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9887\n",
            "Epoch 167/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0320 - accuracy: 0.9907\n",
            "Epoch 168/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9873\n",
            "Epoch 169/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9850\n",
            "Epoch 170/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0430 - accuracy: 0.9863\n",
            "Epoch 171/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9853\n",
            "Epoch 172/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0431 - accuracy: 0.9867\n",
            "Epoch 173/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.9930\n",
            "Epoch 174/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9910\n",
            "Epoch 175/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.9943\n",
            "Epoch 176/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.9953\n",
            "Epoch 177/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.9930\n",
            "Epoch 178/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.9933\n",
            "Epoch 179/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 0.9883\n",
            "Epoch 180/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0429 - accuracy: 0.9873\n",
            "Epoch 181/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9867\n",
            "Epoch 182/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.9923\n",
            "Epoch 183/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.9937\n",
            "Epoch 184/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0362 - accuracy: 0.9923\n",
            "Epoch 185/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9957\n",
            "Epoch 186/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.9920\n",
            "Epoch 187/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0401 - accuracy: 0.9883\n",
            "Epoch 188/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0374 - accuracy: 0.9893\n",
            "Epoch 189/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.9927\n",
            "Epoch 190/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0391 - accuracy: 0.9887\n",
            "Epoch 191/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9923\n",
            "Epoch 192/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.9927\n",
            "Epoch 193/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.9923\n",
            "Epoch 194/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.9907\n",
            "Epoch 195/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0361 - accuracy: 0.9890\n",
            "Epoch 196/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.9880\n",
            "Epoch 197/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.9920\n",
            "Epoch 198/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.9950\n",
            "Epoch 199/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0403 - accuracy: 0.9870\n",
            "Epoch 200/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.9943\n",
            "Epoch 201/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.9947\n",
            "Epoch 202/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.9937\n",
            "Epoch 203/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.9893\n",
            "Epoch 204/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.9950\n",
            "Epoch 205/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.9927\n",
            "Epoch 206/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0355 - accuracy: 0.9893\n",
            "Epoch 207/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9837\n",
            "Epoch 208/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.9860\n",
            "Epoch 209/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0433 - accuracy: 0.9887\n",
            "Epoch 210/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.9903\n",
            "Epoch 211/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.9947\n",
            "Epoch 212/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.9950\n",
            "Epoch 213/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 0.9873\n",
            "Epoch 214/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9893\n",
            "Epoch 215/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9923\n",
            "Epoch 216/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.9923\n",
            "Epoch 217/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.9897\n",
            "Epoch 218/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0427 - accuracy: 0.9883\n",
            "Epoch 219/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9893\n",
            "Epoch 220/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.9937\n",
            "Epoch 221/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.9950\n",
            "Epoch 222/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.9937\n",
            "Epoch 223/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.9907\n",
            "Epoch 224/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.9930\n",
            "Epoch 225/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.9937\n",
            "Epoch 226/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9957\n",
            "Epoch 227/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.9950\n",
            "Epoch 228/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.9933\n",
            "Epoch 229/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.9927\n",
            "Epoch 230/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.9927\n",
            "Epoch 231/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9950\n",
            "Epoch 232/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.9950\n",
            "Epoch 233/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.9917\n",
            "Epoch 234/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 0.9873\n",
            "Epoch 235/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 0.9873\n",
            "Epoch 236/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.9903\n",
            "Epoch 237/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9863\n",
            "Epoch 238/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0355 - accuracy: 0.9893\n",
            "Epoch 239/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.9903\n",
            "Epoch 240/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9923\n",
            "Epoch 241/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.9963\n",
            "Epoch 242/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9973\n",
            "Epoch 243/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 0.9950\n",
            "Epoch 244/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0321 - accuracy: 0.9907\n",
            "Epoch 245/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.9917\n",
            "Epoch 246/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9947\n",
            "Epoch 247/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9970\n",
            "Epoch 248/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.9940\n",
            "Epoch 249/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.9947\n",
            "Epoch 250/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0169 - accuracy: 0.9933\n",
            "Epoch 251/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.9933\n",
            "Epoch 252/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9970\n",
            "Epoch 253/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.9913\n",
            "Epoch 254/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.9893\n",
            "Epoch 255/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.9920\n",
            "Epoch 256/256\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0150 - accuracy: 0.9950\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x=dig_train['data'], y=dig_train['target'], \n",
        "                    epochs=model_sel.loc[best_index, 'epochs'],\n",
        "                    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tovQW_Llc1Fw"
      },
      "source": [
        "## Test Model on Test Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1043,
          "status": "ok",
          "timestamp": 1677084326156,
          "user": {
            "displayName": "Ulrich Bodenhofer",
            "userId": "07009582510819085803"
          },
          "user_tz": -60
        },
        "id": "5g81CNxUc2Z8",
        "outputId": "f13ee303-f283-4e67-b664-477078fdcf8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 2ms/step\n",
            "[[208   3   0   0   0   0   1   0   0   0]\n",
            " [  0 200   1   0   1   0   0   1   0   0]\n",
            " [  0   2 183   0   0   0   0   0   0   1]\n",
            " [  1   2   1 190   1   4   0   1   0   2]\n",
            " [  0   4   0   0 202   0   1   0   0   1]\n",
            " [  1   2   0   1   0 179   7   0   0   0]\n",
            " [  2   2   0   0   2   2 195   0   1   0]\n",
            " [  0   2   1   1   0   0   0 195   1   2]\n",
            " [  2   7   0   3   1   1   2   1 173   3]\n",
            " [  2  10   2   3   1   3   0   0   0 183]]\n",
            "\n",
            "\n",
            "Class 0:\n",
            "    Sensitivity (TPR):  98.113% (208 of 212)\n",
            "    Specificity (TNR):  99.554% (1784 of 1792)\n",
            "    Precision:          96.296% (208 of 216)\n",
            "    Neg. pred. value:   99.776% (1784 of 1788)\n",
            "Class 1:\n",
            "    Sensitivity (TPR):  98.522% (200 of 203)\n",
            "    Specificity (TNR):  98.112% (1767 of 1801)\n",
            "    Precision:          85.470% (200 of 234)\n",
            "    Neg. pred. value:   99.831% (1767 of 1770)\n",
            "Class 2:\n",
            "    Sensitivity (TPR):  98.387% (183 of 186)\n",
            "    Specificity (TNR):  99.725% (1813 of 1818)\n",
            "    Precision:          97.340% (183 of 188)\n",
            "    Neg. pred. value:   99.835% (1813 of 1816)\n",
            "Class 3:\n",
            "    Sensitivity (TPR):  94.059% (190 of 202)\n",
            "    Specificity (TNR):  99.556% (1794 of 1802)\n",
            "    Precision:          95.960% (190 of 198)\n",
            "    Neg. pred. value:   99.336% (1794 of 1806)\n",
            "Class 4:\n",
            "    Sensitivity (TPR):  97.115% (202 of 208)\n",
            "    Specificity (TNR):  99.666% (1790 of 1796)\n",
            "    Precision:          97.115% (202 of 208)\n",
            "    Neg. pred. value:   99.666% (1790 of 1796)\n",
            "Class 5:\n",
            "    Sensitivity (TPR):  94.211% (179 of 190)\n",
            "    Specificity (TNR):  99.449% (1804 of 1814)\n",
            "    Precision:          94.709% (179 of 189)\n",
            "    Neg. pred. value:   99.394% (1804 of 1815)\n",
            "Class 6:\n",
            "    Sensitivity (TPR):  95.588% (195 of 204)\n",
            "    Specificity (TNR):  99.389% (1789 of 1800)\n",
            "    Precision:          94.660% (195 of 206)\n",
            "    Neg. pred. value:   99.499% (1789 of 1798)\n",
            "Class 7:\n",
            "    Sensitivity (TPR):  96.535% (195 of 202)\n",
            "    Specificity (TNR):  99.834% (1799 of 1802)\n",
            "    Precision:          98.485% (195 of 198)\n",
            "    Neg. pred. value:   99.612% (1799 of 1806)\n",
            "Class 8:\n",
            "    Sensitivity (TPR):  89.637% (173 of 193)\n",
            "    Specificity (TNR):  99.890% (1809 of 1811)\n",
            "    Precision:          98.857% (173 of 175)\n",
            "    Neg. pred. value:   98.907% (1809 of 1829)\n",
            "Class 9:\n",
            "    Sensitivity (TPR):  89.706% (183 of 204)\n",
            "    Specificity (TNR):  99.500% (1791 of 1800)\n",
            "    Precision:          95.312% (183 of 192)\n",
            "    Neg. pred. value:   98.841% (1791 of 1812)\n",
            "\n",
            "Overall accuracy:   95.210% (1908 of 2004)\n",
            "Balanced accuracy:  95.187%\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(dig_test['data'])\n",
        "\n",
        "evaluate_classification_result(dig_test['target'], pred);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(\"Pritz_Sebastian.hdf5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python (MLTeaching)",
      "language": "python",
      "name": "mlteaching"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
