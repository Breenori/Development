{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9b704d-1fe0-4131-8f0a-59199305cdec",
   "metadata": {},
   "source": [
    "# Exercise 4: Unstrukturierten zu strukturierten Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702ce3bd-774f-4a42-86d7-c49e37b292de",
   "metadata": {
    "tags": []
   },
   "source": [
    "Aufwand: 5h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65eaf2-dca2-4666-aebb-ec2b595210d0",
   "metadata": {},
   "source": [
    "Implementieren Sie eines natürlichen Sprachalgorithmus, angewandt auf Webeinträge, Blogeinträge oder\n",
    "ähnliche-Postings (für die deutsche Sprache). Beispiele dafür wären (Instagram, Wikipedia, Blog, etc.)\n",
    "Folgende Funktionalitäten sollen dabei implementiert werden:\n",
    "\n",
    "1) Als Eingabeparameter soll eine Datei mit >20 Webeinträgen (Instagram Kommentare, Wikipedia Absätze, Blog Einträge, usw.) verwendet werden. Verwenden Sie hierbei das gelernte Wissen aus der Vorlesung und crawlen Sie die Beiträge, achten Sie darauf, dass der in der Vorlesung verwendete Blog nicht verwendet werden darf. Die Texte sollten unbedingt Emojis enthalten (gegebenenfalls können Emojis selbständig ergänzt werden).\n",
    "2) Textblöcke sollen vorverarbeitet werden, indem Emoticons, Umlaute und sonstige Sonderzeichen entsprechend ersetzt werden. Emoticons sollen mit einem Wort ersetzt werden (z.B.: :-), : ) → happy, gluecklich, freudig).\n",
    "3) Die Wörter eines Satzes sollen in ihren Wortstamm und semantische Bedeutung zurückgeführt werden. Sie können dafür gerne bereits ausimplementierte Stemming-Algorithmen verwenden. Führen Sie dazu eine ausführliche Literaturrecherche durch. (Hinweise: TreeTager1, Porter Stemming Algorithmus2). Speichern Sie die Ergebnisse in einer Textdatei ab.\n",
    "4) Nachdem die unstrukturierten Texte entsprechend vorverarbeitet wurden, sollen Synonyme identifiziert werden unter Verwendung der Nomenklatur von Openthesaurus3 (Diese Datei wird Ihnen zur Verfügung gestellt – openthesaurus.txt).\n",
    "5) Auszugeben ist eine csv Datei, welche folgende Spalten enthält: (1) Original-Satz, (2) vorverarbeiteter Satz (Emoticons, Umlaute, Sonderzeichen), (3) gestemmter Satz, (4) Referenzliste mit Synonymen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc06b664-904d-473c-a694-eae00d30fb3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Literaturrecherche\n",
    "* TreeTagger: TreeTagger annotiert Text mit POS und Lemmatization und ist für viele Sprachen verfügbar, darunter Deutsch und Englisch. Die Methode basiert auf probabilistischen Modellen, genauer gesagt Markov Modelle und Decision Trees.\n",
    "* Porter-Stemming Algorithmus: Ist regelbasiert und überprüft mittels Bedingungen, ob ein Wort einen gewissen Suffix enthält, der entfernt werden kann, sodass ein noch \"valides\" aber womöglicherweise nicht existierendes Wort entsteht. Die Regeln basieren allesamt auf der Unterteilung eines Wortes in Vokale und Konsonanten. Es gibt eine Erweiterung des Algorithmus namens \"Snowball\", die das Wort von rechts nach links durchläuft und den längsten möglichen Suffix sucht und entfernt, aber auch auf die vorangehenden Buchstaben achtet, und somit eventuell weniger lange Suffixe entfernt, um richtige Wörter zu erhalten bzw. Überschneidungen zu vermeiden (z.B. \"cared\" würde zu \"car\" werden, was sich mit \"Auto\" überschneidet. Bei Snowball bleibt es \"care\", was dem Zweck entspricht)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2383f1a1-879c-4ef1-a398-58dafd7a559a",
   "metadata": {},
   "source": [
    "## Implementierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ced81d-add8-4a33-9a50-c2efcf8259ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "import re\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "import sys\n",
    "st = GermanStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869df7f0-00ec-45d4-97c8-1495390c4d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = r'https://www.reddit.com/r/de/comments/12o4vat/gew%C3%BCrzketchupbrunnen/'\n",
    "\n",
    "# define headers, so reddit actually returns something\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; zh-CN) AppleWebKit/533+ (KHTML, like Gecko)'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c9ce492-97a3-4786-a628-d5b987e45e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_thesaurus(filepath):\n",
    "    synonym_list = []\n",
    "    with open(filepath,'r', encoding='utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            if not line.startswith(\"#\"):\n",
    "                # preproccess each element by removing special chars and stuff in parentheses\n",
    "                synonyms = line.split(\";\")\n",
    "                synonyms = [re.sub(r\"\\([^);]+\\)\", \"\", synonym) for synonym in synonyms]\n",
    "                synonyms = [re.sub(\"[\\W_]\", \" \", synonym) for synonym in synonyms]\n",
    "                synonyms = [re.sub(r\"\\s+\", \" \", synonym) for synonym in synonyms]\n",
    "                synonyms = [re.sub(\"ä\", \"ae\", synonym) for synonym in synonyms]\n",
    "                synonyms = [re.sub(\"ü\", \"ue\", synonym) for synonym in synonyms]\n",
    "                synonyms = [re.sub(\"ö\", \"oe\", synonym) for synonym in synonyms]\n",
    "                synonyms = [synonym.strip().lower() for synonym in synonyms]\n",
    "                synonym_list.append(synonyms)\n",
    "    return synonym_list    \n",
    "\n",
    "# method to easily fetch the synonyms for a given word\n",
    "def get_synonyms(word, synonym_list):\n",
    "    res = []\n",
    "    for synonyms in synonym_list:\n",
    "        if word in synonyms:\n",
    "            for synonym in synonyms:\n",
    "                if synonym != word:\n",
    "                    res.append(synonym)         \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b1bed5-3c26-4a4c-9c9e-027a969b86ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the thesaurus file.\n",
    "thesaurus = parse_thesaurus(\"openthesaurus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0353ec1e-a9e2-4e6f-ab7b-3100f5f2ac7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "elements = soup.find_all(\"div\", {\"data-testid\": \"comment\"})\n",
    "print(len(elements))\n",
    "\n",
    "with open(\"outfile.csv\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    outfile.write(\"original;preprocessed;stemmed;references\\n\")\n",
    "    for element in elements:\n",
    "        p_res = element.find_all(\"p\")\n",
    "        for p in p_res:\n",
    "            # fetch the content of <p> tag\n",
    "            p = p.get_text()\n",
    "\n",
    "            #Preprocessing. Removing special characters, umlauts, emojis and multiple whitespaces.\n",
    "            preprocessed = emoji.demojize(p.lower(), language=\"de\")\n",
    "            preprocessed = re.sub(\"[\\W_]\", \" \", preprocessed)\n",
    "            preprocessed = re.sub(\"\\s+\", \" \", preprocessed)\n",
    "            preprocessed = re.sub(\"ä\", \"ae\", preprocessed)\n",
    "            preprocessed = re.sub(\"ü\", \"ue\", preprocessed)\n",
    "            preprocessed = re.sub(\"ö\", \"oe\", preprocessed)\n",
    "\n",
    "            # stem it using spaCy\n",
    "            stemmed = \" \".join([st.stem(token) for token in preprocessed.split(\" \")])\n",
    "            \n",
    "            # finally, create a dictionary using the stemmed sentence and the thesaurus\n",
    "            references = dict(zip(stemmed.split(\" \"), [get_synonyms(word, thesaurus) for word in stemmed.split(\" \")]))\n",
    "            outfile.write(f\"{p};{preprocessed};{stemmed};{references}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbcdb1-7d7e-4d06-aecb-cd3911799802",
   "metadata": {},
   "source": [
    "## Notizen\n",
    "Die obige Abfrage returniert leider nur 17 statt den geforderten 20+ Posts. Ich schätze, Reddit rendert gewisse Kommentare erst später durch JavaScript, weshalb sich die Menge hier so drastisch von 200+ auf 17 reduziert. Zeitbedingt habe ich es aber so belassen und hoffe, dass dies kein allzu-großes Problem ist. Zumindest ein Emoji hat es hineingeschafft, womit ich zumindest da Glück hatte :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
